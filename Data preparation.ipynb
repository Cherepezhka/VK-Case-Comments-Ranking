{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Импорт библиотек\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import swifter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.tokenize import regexp_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "from textblob import TextBlob\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "from sklearn.pipeline import Pipeline\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Vovan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Vovan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Vovan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "RND = 322\n",
    "STOPWORDS = set(stopwords.words())\n",
    "PATTERN = r\"[a-zA-Z]+(?:[-'.@&/][a-zA-Z]+)*\"\n",
    "\n",
    "tqdm.pandas()\n",
    "nltk.download([\"punkt\", \"wordnet\", \"stopwords\"]);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подготовка данных\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Предобработку текста будем осуществлять с помощью пайплайна, создав предварительно 4 класса:\n",
    "\n",
    "- TextPreprocessor - создаёт признаки с помощью _nltk_;\n",
    "- TextFeaturesExtractor - создаёт признаки с помощью _pandas_ и _difflib_;\n",
    "- TextBlobFeaturesExtractor - создаёт призкаки средствами библиотеки _textblob_;\n",
    "- DtypeOptimizer - оптимизация типов данных.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextPreprocessor(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Создание новых признаков из текстовых данных с помощью библиотеки NLTK\"\"\"\n",
    "\n",
    "    def __init__(self, pattern: str = PATTERN, stopwords: set = STOPWORDS):\n",
    "        self.pattern = pattern\n",
    "        self.stopwords = stopwords\n",
    "\n",
    "    def tag_words_with_pos(self, words: list) -> list:\n",
    "        \"\"\"\n",
    "        Присваивает теги словам на основе их частей речи.\n",
    "\n",
    "        Параметры:\n",
    "            - words (list): Список слов.\n",
    "\n",
    "        Возвращает:\n",
    "            - list: Список кортежей, содержащих слова и соответствующие теги.\n",
    "        \"\"\"\n",
    "        pos_dict = {\n",
    "            \"J\": wordnet.ADJ,\n",
    "            \"V\": wordnet.VERB,\n",
    "            \"N\": wordnet.NOUN,\n",
    "            \"R\": wordnet.ADV,\n",
    "        }\n",
    "\n",
    "        tagged_words = pos_tag(words)\n",
    "        tagged_words_with_pos = [\n",
    "            (word, pos_dict.get(tag[0], wordnet.NOUN)) for (word, tag) in tagged_words\n",
    "        ]\n",
    "\n",
    "        return tagged_words_with_pos\n",
    "\n",
    "    def filter_words(self, words: list) -> list:\n",
    "        \"\"\"\n",
    "        Фильтрует список слов, удаляя стоп-слова.\n",
    "\n",
    "        Параметры:\n",
    "            - words (list): Список слов.\n",
    "\n",
    "        Возвращает:\n",
    "            - list: Список отфильтрованных слов.\n",
    "        \"\"\"\n",
    "\n",
    "        filtered_words = [word for word in words if word not in self.stopwords]\n",
    "        return filtered_words\n",
    "\n",
    "    def lemmatize_words(self, words: list) -> list:\n",
    "        \"\"\"\n",
    "        Приводит слова к их базовым формам (леммам) с помощью лемматизации.\n",
    "\n",
    "        Параметры:\n",
    "            - words (list): Список слов.\n",
    "\n",
    "        Возвращает:\n",
    "            - list: Список лемматизированных слов.\n",
    "        \"\"\"\n",
    "\n",
    "        wnl = WordNetLemmatizer()\n",
    "        lemmatized_words = [wnl.lemmatize(word, tag) for word, tag in words]\n",
    "        return lemmatized_words\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X: pd.DataFrame) -> pd.DataFrame:\n",
    "\n",
    "        X = X.copy()\n",
    "\n",
    "        # Проводим токенизацию и удаление стоп слов\n",
    "        posts_tokenized = (\n",
    "            X.loc[::5, \"post_text_fix\"]\n",
    "            .swifter.apply(regexp_tokenize, pattern=PATTERN)\n",
    "            .values\n",
    "        )\n",
    "\n",
    "        X[\"post_text_reg_tok\"] = np.repeat(posts_tokenized, 5)\n",
    "\n",
    "        posts_tok_clr = (\n",
    "            X.loc[::5, \"post_text_reg_tok\"].swifter.apply(\n",
    "                self.filter_words).values\n",
    "        )\n",
    "\n",
    "        X[\"post_text_tok_clr\"] = np.repeat(posts_tok_clr, 5)\n",
    "\n",
    "        X[\"comment_text_reg_tok\"] = X[\"comment_text_fix\"].swifter.apply(\n",
    "            regexp_tokenize, pattern=PATTERN\n",
    "        )\n",
    "\n",
    "        X[\"comment_text_tok_clr\"] = X[\"comment_text_reg_tok\"].swifter.apply(\n",
    "            self.filter_words\n",
    "        )\n",
    "\n",
    "        # Простановка частей речи\n",
    "        posts_text_tcm = (\n",
    "            X.loc[::5, \"post_text_tok_clr\"]\n",
    "            .swifter.apply(self.tag_words_with_pos)\n",
    "            .values\n",
    "        )\n",
    "        X[\"post_text_tcm\"] = np.repeat(posts_text_tcm, 5)\n",
    "        X[\"comment_text_tcm\"] = X[\"comment_text_tok_clr\"].swifter.apply(\n",
    "            self.tag_words_with_pos\n",
    "        )\n",
    "\n",
    "        # Лемматизация текста\n",
    "\n",
    "        post_text_lemmatized = (\n",
    "            X.loc[::5, \"post_text_tcm\"].swifter.apply(\n",
    "                self.lemmatize_words).values\n",
    "        )\n",
    "\n",
    "        X[\"post_text_lemmatized\"] = np.repeat(post_text_lemmatized, 5)\n",
    "        X[\"comment_text_lemmatized\"] = X[\"comment_text_tcm\"].swifter.apply(\n",
    "            self.lemmatize_words\n",
    "        )\n",
    "\n",
    "        return X\n",
    "\n",
    "\n",
    "class TextFeaturesExtractor(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Создание новых признаков из текстовых данных с помощью библиотеки pandas и difflib\n",
    "    \"\"\"\n",
    "\n",
    "    def sequence_matcher(self, row: pd.Series) -> float:\n",
    "        \"\"\"\n",
    "        Вычисляет косинусное сходство (cosine similarity) между двумя текстовыми строками с использованием SequenceMatcher.\n",
    "\n",
    "        Параметры:\n",
    "            - row (pd.Series): Строка данных, содержащая текстовые поля \"post_text_fix\" и \"comment_text_fix\".\n",
    "\n",
    "        Возвращает:\n",
    "            - float: Значение косинусного сходства между строками.\n",
    "        \"\"\"\n",
    "        cos_sim = SequenceMatcher(\n",
    "            a=row[\"post_text_fix\"],\n",
    "            b=row[\"comment_text_fix\"],\n",
    "        ).ratio()\n",
    "        return cos_sim\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        # Определение длины строки\n",
    "        X[\"post_text_len\"] = X[\"post_text_fix\"].str.len()\n",
    "        X[\"comment_text_len\"] = X[\"comment_text_fix\"].str.len()\n",
    "        # Определение количества слов\n",
    "        X[\"post_text_words_count\"] = X[\"post_text_reg_tok\"].apply(len)\n",
    "        X[\"comment_text_words_count\"] = X[\"comment_text_reg_tok\"].apply(len)\n",
    "        # Определение количества ссылок в строке\n",
    "        X[\"post_text_links_count\"] = X[\"post_text_fix\"].str.count(r\"\\burl\\b\")\n",
    "        X[\"comment_text_links_count\"] = X[\"comment_text_fix\"].str.count(\n",
    "            r\"\\burl\\b\")\n",
    "        # Определение косинусного сходства между текстом поста и комментария\n",
    "        X[\"cosine_similarity\"] = X.swifter.apply(self.sequence_matcher, axis=1)\n",
    "\n",
    "        return X\n",
    "\n",
    "\n",
    "class TextBlobFeaturesExtractor(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Создание новых признаков из текстовых данных с помощью библиотеки textblob\n",
    "    \"\"\"\n",
    "\n",
    "    def text_polarity(self, text: str) -> tuple(float, float, int):\n",
    "        \"\"\"\n",
    "        Вычисляет количество предложений, полярность и субъективность текста с использованием TextBlob.\n",
    "\n",
    "        Параметры:\n",
    "            - text (str): Текст для анализа.\n",
    "\n",
    "        Возвращает:\n",
    "            - Tuple[float, float, int]: Кортеж с полярностью, субъективностью и количеством предложений.\n",
    "        \"\"\"\n",
    "        blob = TextBlob(text)\n",
    "        polarity = blob.sentiment.polarity\n",
    "        subjectivity = blob.sentiment.subjectivity\n",
    "        line_count = len(blob.sentences)\n",
    "        return polarity, subjectivity, line_count\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "\n",
    "        # Добавляем колонки для комментариев\n",
    "        (\n",
    "            X[\"comment_text_polarity\"],\n",
    "            X[\"comment_text_subjectivity\"],\n",
    "            X[\"comment_text_line_count\"],\n",
    "        ) = zip(*X[\"comment_text_fix\"].swifter.apply(self.text_polarity))\n",
    "\n",
    "        # Добавляем колонки для постов\n",
    "        (\n",
    "            X[\"post_text_polarity\"],\n",
    "            X[\"post_text_subjectivity\"],\n",
    "            X[\"post_text_line_count\"],\n",
    "        ) = zip(*X[\"post_text_fix\"].swifter.apply(self.text_polarity))\n",
    "\n",
    "        return X\n",
    "\n",
    "\n",
    "class DtypeOptimizer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Оптимизация целочисленных типов данных в pd.DataFrame\"\"\"\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        for col in X.select_dtypes(\"int\").columns:\n",
    "            min_val = X[col].min()\n",
    "            max_val = X[col].max()\n",
    "            # проверка беззнаковых типов\n",
    "            if min_val >= 0:\n",
    "                if max_val < np.iinfo(np.uint8).max:\n",
    "                    X[col] = X[col].astype(\"uint8\")\n",
    "                elif max_val < np.iinfo(np.uint16).max:\n",
    "                    X[col] = X[col].astype(\"uint16\")\n",
    "                elif max_val < np.iinfo(np.uint32).max:\n",
    "                    X[col] = X[col].astype(\"uint32\")\n",
    "\n",
    "            elif min_val >= np.iinfo(np.int8).min and max_val < np.iinfo(np.int8).max:\n",
    "                X[col] = X[col].astype(\"int8\")\n",
    "            elif min_val >= np.iinfo(np.int16).min and max_val < np.iinfo(np.int16).max:\n",
    "                X[col] = X[col].astype(\"int16\")\n",
    "            elif (\n",
    "                min_val >= -\n",
    "                    np.iinfo(np.int32).min and max_val < np.iinfo(np.int32).max\n",
    "            ):\n",
    "                X[col] = X[col].astype(\"int32\")\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline(\n",
    "    [\n",
    "        (\"nltk\", TextPreprocessor()),\n",
    "        (\"pandas_str\", TextFeaturesExtractor()),\n",
    "        (\"text_blob\", TextBlobFeaturesExtractor()),\n",
    "        (\"opt_dtypes\", DtypeOptimizer()),\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подгружаем предварительно очищенные данные\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 440535 entries, 0 to 440534\n",
      "Data columns (total 6 columns):\n",
      " #   Column            Non-Null Count   Dtype \n",
      "---  ------            --------------   ----- \n",
      " 0   post_index        440535 non-null  int64 \n",
      " 1   post_text         440535 non-null  object\n",
      " 2   comment_text      440535 non-null  object\n",
      " 3   comment_score     440535 non-null  int64 \n",
      " 4   post_text_fix     440535 non-null  object\n",
      " 5   comment_text_fix  440535 non-null  object\n",
      "dtypes: int64(2), object(4)\n",
      "memory usage: 20.2+ MB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 70020 entries, 0 to 70019\n",
      "Data columns (total 6 columns):\n",
      " #   Column            Non-Null Count  Dtype \n",
      "---  ------            --------------  ----- \n",
      " 0   post_index        70020 non-null  int64 \n",
      " 1   post_text         70020 non-null  object\n",
      " 2   comment_text      70020 non-null  object\n",
      " 3   comment_score     0 non-null      object\n",
      " 4   post_text_fix     70020 non-null  object\n",
      " 5   comment_text_fix  70020 non-null  object\n",
      "dtypes: int64(1), object(5)\n",
      "memory usage: 3.2+ MB\n"
     ]
    }
   ],
   "source": [
    "df_train = pd.read_feather(\"data/df_train_BERT.feather\")\n",
    "df_test = pd.read_feather(\"data/df_test_BERT.feather\")\n",
    "df_train.info()\n",
    "df_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_index</th>\n",
       "      <th>post_text</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>comment_score</th>\n",
       "      <th>post_text_fix</th>\n",
       "      <th>comment_text_fix</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>iOS 8.0.1 released, broken on iPhone 6 models,...</td>\n",
       "      <td>I am still waiting for them to stabilize wifi ...</td>\n",
       "      <td>None</td>\n",
       "      <td>ios 8.0.1 released, broken on iphone 6 models,...</td>\n",
       "      <td>i am still waiting for them to stabilize wifi ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>iOS 8.0.1 released, broken on iPhone 6 models,...</td>\n",
       "      <td>For those who upgraded, no need to do a restor...</td>\n",
       "      <td>None</td>\n",
       "      <td>ios 8.0.1 released, broken on iphone 6 models,...</td>\n",
       "      <td>for those who upgraded, no need to do a restor...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   post_index                                          post_text   \n",
       "0           0  iOS 8.0.1 released, broken on iPhone 6 models,...  \\\n",
       "1           0  iOS 8.0.1 released, broken on iPhone 6 models,...   \n",
       "\n",
       "                                        comment_text comment_score   \n",
       "0  I am still waiting for them to stabilize wifi ...          None  \\\n",
       "1  For those who upgraded, no need to do a restor...          None   \n",
       "\n",
       "                                       post_text_fix   \n",
       "0  ios 8.0.1 released, broken on iphone 6 models,...  \\\n",
       "1  ios 8.0.1 released, broken on iphone 6 models,...   \n",
       "\n",
       "                                    comment_text_fix  \n",
       "0  i am still waiting for them to stabilize wifi ...  \n",
       "1  for those who upgraded, no need to do a restor...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display(df_train.head(2))\n",
    "display(df_test.head(2))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Применяем обработку текста и сохраняем данные для обучающей и тестовой выборки в файлы\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df_train_transformed = pipe.fit_transform(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 1min 13s\n",
      "Wall time: 1min 7s\n"
     ]
    }
   ],
   "source": [
    "df_train_transformed.to_parquet(\n",
    "    \"data/df_train_with_features.parquet\", engine=\"pyarrow\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df_test_transformed = pipe.transform(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 20.5 s\n",
      "Wall time: 18.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_test_transformed.to_parquet(\n",
    "    \"data/df_test_with_features.parquet\", engine=\"pyarrow\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
